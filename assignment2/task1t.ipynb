{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.23.4-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.2-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\machine\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\machine\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\machine\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.6-cp38-cp38-win_amd64.whl (163 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\machine\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.3.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, pytz, pillow, kiwisolver, fonttools, cycler, contourpy, sklearn, pandas, matplotlib\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 numpy-1.23.4 pandas-1.5.1 pillow-9.3.0 pytz-2022.6 sklearn-0.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\machine\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas numpy sklearn matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0    male        group A                 high school      standard   \n",
       "1  female        group D            some high school  free/reduced   \n",
       "2    male        group E                some college  free/reduced   \n",
       "3    male        group B                 high school      standard   \n",
       "4    male        group E          associate's degree      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0               completed          67             67             63  \n",
       "1                    none          40             59             55  \n",
       "2                    none          59             60             50  \n",
       "3                    none          77             78             68  \n",
       "4               completed          78             73             68  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('Task 12.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# encode gender column using sklearn\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      4\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(df[\u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m      5\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mrace/ethnicity\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(df[\u001b[39m\"\u001b[39m\u001b[39mrace/ethnicity\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# encode gender column using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df[\"gender\"] = LabelEncoder().fit_transform(df[\"gender\"].values)\n",
    "df[\"race/ethnicity\"] = LabelEncoder().fit_transform(df[\"race/ethnicity\"].values)\n",
    "df[\"parental level of education\"] = LabelEncoder().fit_transform(\n",
    "    df[\"parental level of education\"].values)\n",
    "df[\"lunch\"] = LabelEncoder().fit_transform(df[\"lunch\"].values)\n",
    "df[\"test preparation course\"] = LabelEncoder().fit_transform(\n",
    "    df[\"test preparation course\"].values)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  race/ethnicity  parental level of education  lunch  \\\n",
       "0       1               0                            2      1   \n",
       "1       0               3                            5      0   \n",
       "2       1               4                            4      0   \n",
       "3       1               1                            2      1   \n",
       "4       1               4                            0      1   \n",
       "\n",
       "   test preparation course  math score  reading score  writing score  \n",
       "0                        0          67             67             63  \n",
       "1                        1          40             59             55  \n",
       "2                        1          59             60             50  \n",
       "3                        1          77             78             68  \n",
       "4                        0          78             73             68  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# DummyScaler = FunctionTransformer(lambda x: x)\n",
    "\n",
    "# scalers1 = [\n",
    "#     MinMaxScaler(),\n",
    "#     MinMaxScaler(),\n",
    "#     MinMaxScaler()\n",
    "# ]\n",
    "\n",
    "# for i, col in enumerate([\"math score\", \"reading score\", \"writing score\"]):\n",
    "#     df[col] = scalers1[i].fit_transform(df[[col]])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 5), (1000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get the target columns\n",
    "target = df[[\"math score\", \"reading score\", \"writing score\"]].values\n",
    "# drop the target columns\n",
    "df = df.drop([\"math score\", \"reading score\", \"writing score\"], axis=1).values\n",
    "\n",
    "X, y = df, target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=5, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc5): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the model\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss = 4698.9150\n",
      "Epoch: 20, Loss = 3672.9226\n",
      "Epoch: 30, Loss = 787.0406\n",
      "Epoch: 40, Loss = 641.1218\n",
      "Epoch: 50, Loss = 555.3272\n",
      "Epoch: 60, Loss = 414.7167\n",
      "Epoch: 70, Loss = 341.3186\n",
      "Epoch: 80, Loss = 305.0911\n",
      "Epoch: 90, Loss = 282.1265\n",
      "Epoch: 100, Loss = 268.2711\n",
      "Epoch: 110, Loss = 258.9629\n",
      "Epoch: 120, Loss = 250.8335\n",
      "Epoch: 130, Loss = 243.7449\n",
      "Epoch: 140, Loss = 237.6117\n",
      "Epoch: 150, Loss = 231.8015\n",
      "Epoch: 160, Loss = 225.9989\n",
      "Epoch: 170, Loss = 219.7914\n",
      "Epoch: 180, Loss = 213.7625\n",
      "Epoch: 190, Loss = 208.3211\n",
      "Epoch: 200, Loss = 203.8020\n",
      "Epoch: 210, Loss = 199.6441\n",
      "Epoch: 220, Loss = 195.9689\n",
      "Epoch: 230, Loss = 192.8620\n",
      "Epoch: 240, Loss = 190.2900\n",
      "Epoch: 250, Loss = 188.0714\n",
      "Epoch: 260, Loss = 186.1018\n",
      "Epoch: 270, Loss = 184.4258\n",
      "Epoch: 280, Loss = 183.0124\n",
      "Epoch: 290, Loss = 181.7336\n",
      "Epoch: 300, Loss = 180.4999\n",
      "Epoch: 310, Loss = 179.4509\n",
      "Epoch: 320, Loss = 178.5440\n",
      "Epoch: 330, Loss = 177.7379\n",
      "Epoch: 340, Loss = 176.9523\n",
      "Epoch: 350, Loss = 176.2908\n",
      "Epoch: 360, Loss = 175.7308\n",
      "Epoch: 370, Loss = 175.2203\n",
      "Epoch: 380, Loss = 174.7623\n",
      "Epoch: 390, Loss = 174.3203\n",
      "Epoch: 400, Loss = 173.9084\n",
      "Epoch: 410, Loss = 173.4692\n",
      "Epoch: 420, Loss = 172.9524\n",
      "Epoch: 430, Loss = 172.3473\n",
      "Epoch: 440, Loss = 171.6595\n",
      "Epoch: 450, Loss = 170.8798\n",
      "Epoch: 460, Loss = 170.0095\n",
      "Epoch: 470, Loss = 169.0266\n",
      "Epoch: 480, Loss = 167.9413\n",
      "Epoch: 490, Loss = 166.7451\n",
      "Epoch: 500, Loss = 165.4532\n",
      "Epoch: 510, Loss = 164.1286\n",
      "Epoch: 520, Loss = 162.9033\n",
      "Epoch: 530, Loss = 161.9244\n",
      "Epoch: 540, Loss = 161.2620\n",
      "Epoch: 550, Loss = 160.8332\n",
      "Epoch: 560, Loss = 160.5235\n",
      "Epoch: 570, Loss = 160.2871\n",
      "Epoch: 580, Loss = 160.0907\n",
      "Epoch: 590, Loss = 159.9084\n",
      "Epoch: 600, Loss = 159.7384\n",
      "Epoch: 610, Loss = 159.5775\n",
      "Epoch: 620, Loss = 159.4176\n",
      "Epoch: 630, Loss = 159.2711\n",
      "Epoch: 640, Loss = 159.1251\n",
      "Epoch: 650, Loss = 158.9862\n",
      "Epoch: 660, Loss = 158.8514\n",
      "Epoch: 670, Loss = 158.7227\n",
      "Epoch: 680, Loss = 158.5966\n",
      "Epoch: 690, Loss = 158.4726\n",
      "Epoch: 700, Loss = 158.3489\n",
      "Epoch: 710, Loss = 158.2268\n",
      "Epoch: 720, Loss = 158.0998\n",
      "Epoch: 730, Loss = 157.9688\n",
      "Epoch: 740, Loss = 157.8317\n",
      "Epoch: 750, Loss = 157.6806\n",
      "Epoch: 760, Loss = 157.5301\n",
      "Epoch: 770, Loss = 157.3978\n",
      "Epoch: 780, Loss = 157.2562\n",
      "Epoch: 790, Loss = 157.1244\n",
      "Epoch: 800, Loss = 156.9975\n",
      "Epoch: 810, Loss = 156.8673\n",
      "Epoch: 820, Loss = 156.7398\n",
      "Epoch: 830, Loss = 156.6100\n",
      "Epoch: 840, Loss = 156.4740\n",
      "Epoch: 850, Loss = 156.3336\n",
      "Epoch: 860, Loss = 156.1980\n",
      "Epoch: 870, Loss = 156.0725\n",
      "Epoch: 880, Loss = 155.9343\n",
      "Epoch: 890, Loss = 155.8000\n",
      "Epoch: 900, Loss = 155.6621\n",
      "Epoch: 910, Loss = 155.5344\n",
      "Epoch: 920, Loss = 155.3973\n",
      "Epoch: 930, Loss = 155.2591\n",
      "Epoch: 940, Loss = 155.1325\n",
      "Epoch: 950, Loss = 154.9849\n",
      "Epoch: 960, Loss = 154.8477\n",
      "Epoch: 970, Loss = 154.6927\n",
      "Epoch: 980, Loss = 154.5488\n",
      "Epoch: 990, Loss = 154.3894\n",
      "Epoch: 1000, Loss = 154.2101\n",
      "Epoch: 1010, Loss = 154.0379\n",
      "Epoch: 1020, Loss = 153.8699\n",
      "Epoch: 1030, Loss = 153.6950\n",
      "Epoch: 1040, Loss = 153.5207\n",
      "Epoch: 1050, Loss = 153.3205\n",
      "Epoch: 1060, Loss = 153.1089\n",
      "Epoch: 1070, Loss = 152.8855\n",
      "Epoch: 1080, Loss = 152.6844\n",
      "Epoch: 1090, Loss = 152.4753\n",
      "Epoch: 1100, Loss = 152.2760\n",
      "Epoch: 1110, Loss = 152.1192\n",
      "Epoch: 1120, Loss = 151.8736\n",
      "Epoch: 1130, Loss = 151.6735\n",
      "Epoch: 1140, Loss = 151.5503\n",
      "Epoch: 1150, Loss = 151.2792\n",
      "Epoch: 1160, Loss = 151.0813\n",
      "Epoch: 1170, Loss = 150.8852\n",
      "Epoch: 1180, Loss = 150.6905\n",
      "Epoch: 1190, Loss = 150.4844\n",
      "Epoch: 1200, Loss = 150.2612\n",
      "Epoch: 1210, Loss = 150.0691\n",
      "Epoch: 1220, Loss = 149.8677\n",
      "Epoch: 1230, Loss = 149.6804\n",
      "Epoch: 1240, Loss = 149.4529\n",
      "Epoch: 1250, Loss = 149.2219\n",
      "Epoch: 1260, Loss = 149.0102\n",
      "Epoch: 1270, Loss = 148.7769\n",
      "Epoch: 1280, Loss = 148.5183\n",
      "Epoch: 1290, Loss = 148.2521\n",
      "Epoch: 1300, Loss = 148.0267\n",
      "Epoch: 1310, Loss = 147.8228\n",
      "Epoch: 1320, Loss = 147.5377\n",
      "Epoch: 1330, Loss = 147.2943\n",
      "Epoch: 1340, Loss = 147.0810\n",
      "Epoch: 1350, Loss = 147.0170\n",
      "Epoch: 1360, Loss = 146.6467\n",
      "Epoch: 1370, Loss = 146.3890\n",
      "Epoch: 1380, Loss = 146.1950\n",
      "Epoch: 1390, Loss = 145.9569\n",
      "Epoch: 1400, Loss = 145.6711\n",
      "Epoch: 1410, Loss = 145.4429\n",
      "Epoch: 1420, Loss = 145.2496\n",
      "Epoch: 1430, Loss = 145.0831\n",
      "Epoch: 1440, Loss = 144.7321\n",
      "Epoch: 1450, Loss = 144.4472\n",
      "Epoch: 1460, Loss = 144.1889\n",
      "Epoch: 1470, Loss = 144.1403\n",
      "Epoch: 1480, Loss = 143.6801\n",
      "Epoch: 1490, Loss = 143.4515\n",
      "Epoch: 1500, Loss = 143.1746\n",
      "Epoch: 1510, Loss = 142.9386\n",
      "Epoch: 1520, Loss = 142.6364\n",
      "Epoch: 1530, Loss = 142.4401\n",
      "Epoch: 1540, Loss = 142.1786\n",
      "Epoch: 1550, Loss = 141.8761\n",
      "Epoch: 1560, Loss = 141.6158\n",
      "Epoch: 1570, Loss = 141.3065\n",
      "Epoch: 1580, Loss = 141.0525\n",
      "Epoch: 1590, Loss = 140.9552\n",
      "Epoch: 1600, Loss = 140.7469\n",
      "Epoch: 1610, Loss = 140.5996\n",
      "Epoch: 1620, Loss = 140.1593\n",
      "Epoch: 1630, Loss = 140.0046\n",
      "Epoch: 1640, Loss = 139.8364\n",
      "Epoch: 1650, Loss = 139.5033\n",
      "Epoch: 1660, Loss = 139.4692\n",
      "Epoch: 1670, Loss = 139.3150\n",
      "Epoch: 1680, Loss = 138.9369\n",
      "Epoch: 1690, Loss = 138.7941\n",
      "Epoch: 1700, Loss = 138.5691\n",
      "Epoch: 1710, Loss = 138.3009\n",
      "Epoch: 1720, Loss = 138.1473\n",
      "Epoch: 1730, Loss = 137.9218\n",
      "Epoch: 1740, Loss = 137.7488\n",
      "Epoch: 1750, Loss = 137.6642\n",
      "Epoch: 1760, Loss = 137.6486\n",
      "Epoch: 1770, Loss = 137.6359\n",
      "Epoch: 1780, Loss = 137.0918\n",
      "Epoch: 1790, Loss = 137.0694\n",
      "Epoch: 1800, Loss = 136.7938\n",
      "Epoch: 1810, Loss = 136.5753\n",
      "Epoch: 1820, Loss = 136.5587\n",
      "Epoch: 1830, Loss = 136.6748\n",
      "Epoch: 1840, Loss = 136.6484\n",
      "Epoch: 1850, Loss = 135.9890\n",
      "Epoch: 1860, Loss = 135.9778\n",
      "Epoch: 1870, Loss = 135.7556\n",
      "Epoch: 1880, Loss = 135.6005\n",
      "Epoch: 1890, Loss = 135.5040\n",
      "Epoch: 1900, Loss = 135.4563\n",
      "Epoch: 1910, Loss = 135.3164\n",
      "Epoch: 1920, Loss = 135.1166\n",
      "Epoch: 1930, Loss = 135.4301\n",
      "Epoch: 1940, Loss = 135.0256\n",
      "Epoch: 1950, Loss = 134.6989\n",
      "Epoch: 1960, Loss = 134.6183\n",
      "Epoch: 1970, Loss = 134.4352\n",
      "Epoch: 1980, Loss = 134.5663\n",
      "Epoch: 1990, Loss = 134.6171\n",
      "Epoch: 2000, Loss = 134.1494\n",
      "Epoch: 2010, Loss = 133.8501\n",
      "Epoch: 2020, Loss = 133.8176\n",
      "Epoch: 2030, Loss = 134.0424\n",
      "Epoch: 2040, Loss = 134.0189\n",
      "Epoch: 2050, Loss = 133.6056\n",
      "Epoch: 2060, Loss = 133.2903\n",
      "Epoch: 2070, Loss = 133.3249\n",
      "Epoch: 2080, Loss = 133.5387\n",
      "Epoch: 2090, Loss = 133.3986\n",
      "Epoch: 2100, Loss = 133.1975\n",
      "Epoch: 2110, Loss = 133.1032\n",
      "Epoch: 2120, Loss = 132.8074\n",
      "Epoch: 2130, Loss = 133.0060\n",
      "Epoch: 2140, Loss = 132.7193\n",
      "Epoch: 2150, Loss = 132.5558\n",
      "Epoch: 2160, Loss = 132.2477\n",
      "Epoch: 2170, Loss = 132.9064\n",
      "Epoch: 2180, Loss = 133.2364\n",
      "Epoch: 2190, Loss = 132.2215\n",
      "Epoch: 2200, Loss = 132.0005\n",
      "Epoch: 2210, Loss = 132.3579\n",
      "Epoch: 2220, Loss = 132.2281\n",
      "Epoch: 2230, Loss = 131.8762\n",
      "Epoch: 2240, Loss = 131.6708\n",
      "Epoch: 2250, Loss = 131.5384\n",
      "Epoch: 2260, Loss = 131.4576\n",
      "Epoch: 2270, Loss = 131.8342\n",
      "Epoch: 2280, Loss = 131.7548\n",
      "Epoch: 2290, Loss = 131.2866\n",
      "Epoch: 2300, Loss = 131.3366\n",
      "Epoch: 2310, Loss = 131.4799\n",
      "Epoch: 2320, Loss = 131.5171\n",
      "Epoch: 2330, Loss = 131.1139\n",
      "Epoch: 2340, Loss = 131.0694\n",
      "Epoch: 2350, Loss = 131.1929\n",
      "Epoch: 2360, Loss = 130.8911\n",
      "Epoch: 2370, Loss = 131.3687\n",
      "Epoch: 2380, Loss = 131.1826\n",
      "Epoch: 2390, Loss = 130.8748\n",
      "Epoch: 2400, Loss = 130.9263\n",
      "Epoch: 2410, Loss = 130.5263\n",
      "Epoch: 2420, Loss = 131.1741\n",
      "Epoch: 2430, Loss = 131.2151\n",
      "Epoch: 2440, Loss = 130.6472\n",
      "Epoch: 2450, Loss = 130.2872\n",
      "Epoch: 2460, Loss = 130.6481\n",
      "Epoch: 2470, Loss = 130.6787\n",
      "Epoch: 2480, Loss = 130.2165\n",
      "Epoch: 2490, Loss = 130.0621\n",
      "Epoch: 2500, Loss = 130.2019\n",
      "Epoch: 2510, Loss = 130.4352\n",
      "Epoch: 2520, Loss = 129.8959\n",
      "Epoch: 2530, Loss = 131.2516\n",
      "Epoch: 2540, Loss = 130.7089\n",
      "Epoch: 2550, Loss = 130.1463\n",
      "Epoch: 2560, Loss = 129.9145\n",
      "Epoch: 2570, Loss = 129.6685\n",
      "Epoch: 2580, Loss = 130.2658\n",
      "Epoch: 2590, Loss = 130.9890\n",
      "Epoch: 2600, Loss = 129.5593\n",
      "Epoch: 2610, Loss = 129.7617\n",
      "Epoch: 2620, Loss = 129.4935\n",
      "Epoch: 2630, Loss = 129.9672\n",
      "Epoch: 2640, Loss = 129.5418\n",
      "Epoch: 2650, Loss = 129.4969\n",
      "Epoch: 2660, Loss = 129.7950\n",
      "Epoch: 2670, Loss = 129.5854\n",
      "Epoch: 2680, Loss = 129.4651\n",
      "Epoch: 2690, Loss = 129.2189\n",
      "Epoch: 2700, Loss = 129.4885\n",
      "Epoch: 2710, Loss = 129.2380\n",
      "Epoch: 2720, Loss = 129.3094\n",
      "Epoch: 2730, Loss = 129.1189\n",
      "Epoch: 2740, Loss = 129.2163\n",
      "Epoch: 2750, Loss = 129.0074\n",
      "Epoch: 2760, Loss = 129.5514\n",
      "Epoch: 2770, Loss = 128.8512\n",
      "Epoch: 2780, Loss = 129.1032\n",
      "Epoch: 2790, Loss = 129.1149\n",
      "Epoch: 2800, Loss = 128.9155\n",
      "Epoch: 2810, Loss = 129.4675\n",
      "Epoch: 2820, Loss = 129.0756\n",
      "Epoch: 2830, Loss = 128.5927\n",
      "Epoch: 2840, Loss = 129.6836\n",
      "Epoch: 2850, Loss = 129.0397\n",
      "Epoch: 2860, Loss = 128.5547\n",
      "Epoch: 2870, Loss = 128.6165\n",
      "Epoch: 2880, Loss = 129.0713\n",
      "Epoch: 2890, Loss = 128.4032\n",
      "Epoch: 2900, Loss = 128.9980\n",
      "Epoch: 2910, Loss = 128.6586\n",
      "Epoch: 2920, Loss = 128.3301\n",
      "Epoch: 2930, Loss = 129.8248\n",
      "Epoch: 2940, Loss = 129.1074\n",
      "Epoch: 2950, Loss = 128.6498\n",
      "Epoch: 2960, Loss = 128.8270\n",
      "Epoch: 2970, Loss = 128.2757\n",
      "Epoch: 2980, Loss = 128.1317\n",
      "Epoch: 2990, Loss = 128.7633\n",
      "Epoch: 3000, Loss = 128.3955\n"
     ]
    }
   ],
   "source": [
    "# define loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# convert to tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(3000):\n",
    "    # forward pass\n",
    "    y_pred = model(X_train)\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1}, Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 180.9404\n",
      "Accuracy: 10.9544\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "with torch.no_grad():\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    y_pred = model(X_test)\n",
    "\n",
    "    loss = criterion(y_pred, y_test)\n",
    "    print(f'Loss: {loss:.4f}')\n",
    "    # find accuracy\n",
    "    acc = (y_pred - y_test).abs().mean()\n",
    "    print(f'Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
